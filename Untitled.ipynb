{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "REFERENCES:\n",
    "General:\n",
    "— \n",
    "\n",
    "Methods that were originally developed and tested by Adobe and Alibaba:\n",
    "— G. Theocharous, P. Thomas, and M. Ghavamzadeh, Personalized Ad Recommendation Systems \n",
    "for Life-Time Value Optimization with Guarantees, 2015;\n",
    "— D. Wu, X. Chen, X. Yang, H. Wang, Q. Tan, X. Zhang, J. Xu, and K. Gai, Budget Constrained \n",
    "Bidding by Model-free Reinforcement Learning in Display Advertising, 2018;\n",
    "\n",
    "Other:\n",
    "— D. Ernst, L. Wehenkel, and P. Geurts, Tree-based batch mode reinforcement learning. \n",
    "Journal of Machine Learning Research, 2005;\n",
    "— M. Riedmiller, Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural \n",
    "Reinforcement Learning Method, 2005;\n",
    "— V. Mnih et al, Human-level Control Through Deep Reinforcement Learning, 2015;\n",
    "\n",
    "OBJECTIVE:\n",
    "A simple testbed environment with synthetic data to explain the model design and \n",
    "implementation more cleanly.\n",
    "\n",
    "THEORY:\n",
    "Basics of customer intent analysis:\n",
    "One of the most basic and widely used targeting methods: look-alike modeling.\n",
    "The idea of look-alike modeling is to personalize ads or offers based on the similarity of a\n",
    "given customer to other customers who have exhibited certain desirable or undesirable \n",
    "properties in the past. \n",
    "\n",
    "We can approach this problem by collecting a number of historical customer profiles that can\n",
    "include both demographic and behavioral features, attributing those features with the \n",
    "observed outcomes, training a classification model based on these samples and then using this\n",
    "model to score any given customer to determine whether the offer should be issued or not.\n",
    "\n",
    "This approach provides significant flexibility, in the sense that models can be built for a \n",
    "wide variety of business objectives depending on how the outcome label is defined. It also \n",
    "works well in a variety of environments, including online advertising and retail promotions.\n",
    "\n",
    "The solution described above can be extended in a number of ways. One of the apparent \n",
    "limitations of basic look-alike modeling is that models for different outcomes and objectives\n",
    "are built separately, and the useful information about similarities between offerings is \n",
    "ignored. \n",
    "\n",
    "This issue becomes critical in environments with a large number of offerings, such as \n",
    "recommendation systems. Typically, the problem is circumvented by incorporating user \n",
    "features, offering features and a larger user-offering interaction matrix into the model so \n",
    "that interaction patterns can be generalized across the offerings; it is done in many \n",
    "collaborative filtering algorithms, including ones that are based on matrix factorization, \n",
    "factorization machines and deep learning methods.\n",
    "\n",
    "These methods help to predict customer intent more accurately by incorporating a wider range\n",
    "of data, but this is not particularly useful for optimizing multi-step action strategies.\n",
    "\n",
    "The problem of strategic optimization can be partly addressed by a more careful design of the\n",
    "target variable, and this group of techniques represents the second important extension of \n",
    "basic look-alike modeling. The target variable is often designed to quantify the probability\n",
    "of some immediate event like a click, purchase, or subscription cancellation, but it can also\n",
    "incorporate more strategic considerations. \n",
    "\n",
    "For example, it is common to combine look-alike modeling with lifetime value (LTV) models to \n",
    "quantify not only the probability of the event, but also its long term economic impact (e.g. \n",
    "what will be the total return from a customer retained from churn or what will be a 3-month \n",
    "spending uplift after a special offer):\n",
    "\n",
    "In this case, for a Profile x, we can say that the probability of each Outcome can  be scored\n",
    "as:\n",
    "\n",
    "Unconditional Propensity: score(x) = P(response | x)\n",
    "Expected LVT: score(x) = P(response | x) * LVT(x)\n",
    "LVT Uplift: score(x) = (P(response | x) - P(response | no offer, x)) * LVT(x)\n",
    "\n",
    "While these techniques help put the modeling process into a strategic context, they do not \n",
    "really provide a framework for optimizing a long-term strategy. So our next step will be to \n",
    "develop a more suitable framework, specifically for this problem.\n",
    "\n",
    "Customer journey as a markov decision process:\n",
    "The problem of strategic (multi-step) optimization stems from the stateful nature of \n",
    "customer relationships and dependencies between actions. For example, one can view retail \n",
    "offer targeting as a single-step process where a customer can either be converted or \n",
    "completely lost (and train a model that maximizes the probability of conversion):\n",
    "\n",
    "Offer —> Purchase\n",
    "  |\n",
    "  ∨\n",
    "Lost costumer\n",
    "\n",
    "However, the real retail environment is more complex and customers can interact with a \n",
    "retailer multiple times before a conversion occurs, and the customer makes related purchases.\n",
    "\n",
    "For instance, in a situation where the actions in the strategy are related, and their order \n",
    "is important, the initial action alone may not increase conversions, but it can boost the \n",
    "efficiency of the downstream actions. \n",
    "\n",
    "It can also be the case that an action is most efficient if issued not in the beginning, but\n",
    "between two other different actions, and so on.\n",
    "\n",
    "In complex structures of a customer journey, as customer maturity grows over time, offerings\n",
    "need to be sequenced properly to account for this.\n",
    "\n",
    "MARKOV DECISION PROCESSES (MDP):\n",
    "Use cases can frequently be represented as Markov Decision Processes (MDP), where a customer\n",
    "can potentially be in several different states and move from one state to another over time \n",
    "under the influence of marketing actions. \n",
    "\n",
    "In each state, the marketer has to choose an action (or non-action) to execute, and each \n",
    "transition between states corresponds to some reward (e.g. number of purchases), so that all\n",
    "rewards along the customer trajectory sum up to a total return that corresponds to customer \n",
    "LTV or campaign ROI.\n",
    "\n",
    "Although we can use a hand-crafted set of states with clear semantic meaning, we will assume\n",
    "the state is represented simply by a customer feature vector, so that the total number of \n",
    "states can be large or infinite in the case of real-valued features. Note that the state at \n",
    "any point of time can include records of all previous actions taken with regard to this \n",
    "customer.\n",
    "\n",
    "In the MDP framework, our goal is to find an optimal policy — π — that maps each state to the \n",
    "probabilities of selecting each possible action, so that π(a | s) is the probability of \n",
    "taking action — a — given that a customer is currently in state — s. The optimality of the \n",
    "policy can be quantified using the expected return under this policy – the expected sum of \n",
    "rewards — r — earned at each transition.\n",
    "\n",
    "A naive solution for this problem is thus to build multiple look-alike models for different \n",
    "actions and different designs of the training labels.\n",
    "\n",
    "In this approach, we first build a set of models (M1.1, M1.2, …) for each action allowed in \n",
    "the initial state, using profiles of customers who received this action in the past and with\n",
    "the target label defined over a long period of time. This label is a direct estimate of the \n",
    "expected return. \n",
    "\n",
    "We then build a second set of models (M2.1, M2.2, …) using customer profiles with features \n",
    "that indicate which action was taken first, and so on. In other words, we optimize actions \n",
    "sequentially using the fact that \"good\" customer trajectories start with \"good\" initial \n",
    "actions for given customer characteristics.\n",
    "\n",
    "However, this naive approach is computationally inefficient, requires building multiple \n",
    "models, and cannot be easily adopted for online learning. We can expect to achieve higher \n",
    "efficiency and flexibility using Reinforcement Learning algorithms that solve a generic MDP \n",
    "problem.\n",
    "\n",
    "SOLVING MDP USING FITTED Q ITERATION\n",
    "In order to apply generic Reinforcement Learning algorithms, we need to introduce a few \n",
    "additional concepts that build on the MDP framework and the notions of state, action, reward\n",
    "and policy that we previously defined.\n",
    "\n",
    "Assuming that we have some policy — π —, let us introduce the action-value function — Qπ — \n",
    "that quantifies the value of taking action — a — in state — s —, as the expected return \n",
    "starting from — s —, taking action — a — and following policy — π.\n",
    "\n",
    "Qπ(s, a) = E(π)[∑_t>ts rt | s, a],\n",
    "\n",
    "Where: \n",
    "E(π) —> Expected-value of policy π;\n",
    "[∑_t>ts rt | s, a] —> SUM (where moment t is after moment ts) of reward at time t given state\n",
    "s, action a.\n",
    "\n",
    "The action-value function can be estimated based on the feedback data (customer trajectories\n",
    "in our case) collected under policy — π. At the same time, the policy itself can be improved\n",
    "based on the estimated action-value function. The most basic way to do this is to simply take\n",
    "the action with the maximum expected value at each state (and, consequently, the \n",
    "probabilities of all other actions in this state will be zero):\n",
    "\n",
    "π(s) = argmax_a xQπ(s, a)\n",
    "\n",
    "allowing for the process of learning the optimal policy to be iterative, as we can start with\n",
    "some baseline policy, collect the feedback data under it, learn the action-value function \n",
    "from this data, update the policy and repeat the cycle again.\n",
    "\n",
    "Next, we need an algorithm for the action-value function estimation. Reinforcement Learning \n",
    "offers a wide range of such algorithms that are designed for different assumptions about the \n",
    "MDP environment. In the most basic case, one can assume a fully known, deterministic and \n",
    "computationally tractable environment, so that all states, actions, and rewards are known. \n",
    "\n",
    "In this case the action-value function can be straightforwardly estimated using classic \n",
    "dynamic programming: we simply compute the total returns for different states as the sums \n",
    "of rewards on the paths in the MDP trellis that originate in these states. \n",
    "\n",
    "It is not even a machine learning problem, but a purely computational problem. However, \n",
    "real environments often impose additional challenges that make this basic approach infeasible\n",
    "and require the development of more advanced and specialized algorithms.\n",
    "\n",
    "Generalization: The number of states or transitions can be very large or infinite. In this \n",
    "case, the training data (trajectories) we have, might not cover all states and we need to \n",
    "generalize our observations using machine learning techniques.\n",
    "\n",
    "Online learning and exploitation: We might have little or no historical data, and learn \n",
    "reward values and states online by trial and error. It leads to the exploration-exploitation\n",
    "problem: the more time we spend on exploring the environment and learning what works and what\n",
    "doesn't, the less time we earn high rewards by making optimal actions according to the best \n",
    "policy we learned.\n",
    "\n",
    "Off-policy evaluation and learning: As previously mentioned, policy learning is generally an\n",
    "incremental process where policy testing and action-value function re-estimation repeats \n",
    "iteratively. In many environments we have a limited ability to test arbitrary policies \n",
    "because they can lead to inappropriate customer experiences or other issues. This means that\n",
    "we might need to learn and evaluate policies based on the feedback data generated under other\n",
    "policies.\n",
    "\n",
    "These challenges are relevant to this use case: the customer state is defined by the vector \n",
    "of real-valued features making the total number of states unlimited, and we generally need to\n",
    "test and adjust our policy on real customers even if we have historical data to start with.\n",
    "\n",
    "The first problem of generalization across the states is addressed by a number of different \n",
    "Reinforcement Learning algorithms. In fact, we've already seen how this generalization can be\n",
    "done using look-alike modeling when we discussed our naive algorithm for multi-step \n",
    "optimization. \n",
    "\n",
    "The more efficient and generic solution for this problem is the Fitted Q Iteration (FQI) \n",
    "algorithm.\n",
    "\n",
    "The idea of the FQI algorithm is to reduce the problem of learning the action-value function\n",
    "to a supervised learning problem. This is done with the iterative application of some \n",
    "supervised learning algorithm; we first train the model to predict the immediate reward based\n",
    "on state and action features, then train the second model to predict the return for two time\n",
    "steps ahead, and so on.\n",
    "\n",
    "More specifically, let us assume that we have a number of trajectories collected under some \n",
    "baseline policy, and we cut these trajectories into a set of individual transitions, each of\n",
    "which is a tuple of the initial state, action, reward and new state:\n",
    "\n",
    "T = {(s, a, r, s')}\n",
    "\n",
    "Where:\n",
    "Input: T -> Set of Transitions\n",
    "Output: Q(s, a) —> Action-Value Fuction\n",
    "\n",
    "We initialize Q(s, a) with some initial function, then repeat the following steps until a \n",
    "convergence condition:\n",
    "\n",
    "1) Initialize empty training set D;\n",
    "2) For each (s_i, a_i, r_i, s'_i) ∈ T:\n",
    "2.1) Calculate q^_i = r_i + γmax_a'Q(s'_i, a'_i);\n",
    "2.2) Add sample ((s_i, a_i), q^_i) to D;\n",
    "3) Learn new Q(s, a) from training set D.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
