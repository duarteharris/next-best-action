{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "REFERENCES:\n",
    "Methods that were originally developed and tested by Adobe and Alibaba:\n",
    "— G. Theocharous, P. Thomas, and M. Ghavamzadeh, Personalized Ad Recommendation Systems \n",
    "for Life-Time Value Optimization with Guarantees, 2015;\n",
    "— D. Wu, X. Chen, X. Yang, H. Wang, Q. Tan, X. Zhang, J. Xu, and K. Gai, Budget Constrained \n",
    "Bidding by Model-free Reinforcement Learning in Display Advertising, 2018;\n",
    "\n",
    "OBJECTIVE:\n",
    "A simple testbed environment with synthetic data to explain the model design and \n",
    "implementation more cleanly.\n",
    "\n",
    "THEORY:\n",
    "Basics of customer intent analysis:\n",
    "One of the most basic and widely used targeting methods: look-alike modeling.\n",
    "The idea of look-alike modeling is to personalize ads or offers based on the similarity of a\n",
    "given customer to other customers who have exhibited certain desirable or undesirable \n",
    "properties in the past. \n",
    "\n",
    "We can approach this problem by collecting a number of historical customer profiles that can\n",
    "include both demographic and behavioral features, attributing those features with the \n",
    "observed outcomes, training a classification model based on these samples and then using this\n",
    "model to score any given customer to determine whether the offer should be issued or not.\n",
    "\n",
    "This approach provides significant flexibility, in the sense that models can be built for a \n",
    "wide variety of business objectives depending on how the outcome label is defined. It also \n",
    "works well in a variety of environments, including online advertising and retail promotions.\n",
    "\n",
    "The solution described above can be extended in a number of ways. One of the apparent \n",
    "limitations of basic look-alike modeling is that models for different outcomes and objectives\n",
    "are built separately, and the useful information about similarities between offerings is \n",
    "ignored. \n",
    "\n",
    "This issue becomes critical in environments with a large number of offerings, such as \n",
    "recommendation systems. Typically, the problem is circumvented by incorporating user \n",
    "features, offering features and a larger user-offering interaction matrix into the model so \n",
    "that interaction patterns can be generalized across the offerings; it is done in many \n",
    "collaborative filtering algorithms, including ones that are based on matrix factorization, \n",
    "factorization machines and deep learning methods.\n",
    "\n",
    "These methods help to predict customer intent more accurately by incorporating a wider range\n",
    "of data, but this is not particularly useful for optimizing multi-step action strategies.\n",
    "\n",
    "The problem of strategic optimization can be partly addressed by a more careful design of the\n",
    "target variable, and this group of techniques represents the second important extension of \n",
    "basic look-alike modeling. The target variable is often designed to quantify the probability\n",
    "of some immediate event like a click, purchase, or subscription cancellation, but it can also\n",
    "incorporate more strategic considerations. \n",
    "\n",
    "For example, it is common to combine look-alike modeling with lifetime value (LTV) models to \n",
    "quantify not only the probability of the event, but also its long term economic impact (e.g. \n",
    "what will be the total return from a customer retained from churn or what will be a 3-month \n",
    "spending uplift after a special offer):\n",
    "\n",
    "In this case, for a Profile x, we can say that the probability of each Outcome can  be scored\n",
    "as:\n",
    "\n",
    "Unconditional Propensity: score(x) = P(response | x)\n",
    "Expected LVT: score(x) = P(response | x) * LVT(x)\n",
    "LVT Uplift: score(x) = (P(response | x) - P(response | no offer, x)) * LVT(x)\n",
    "\n",
    "While these techniques help put the modeling process into a strategic context, they do not \n",
    "really provide a framework for optimizing a long-term strategy. So our next step will be to \n",
    "develop a more suitable framework, specifically for this problem.\n",
    "\n",
    "Customer journey as a markov decision process:\n",
    "The problem of strategic (multi-step) optimization stems from the stateful nature of \n",
    "customer relationships and dependencies between actions. For example, one can view retail \n",
    "offer targeting as a single-step process where a customer can either be converted or \n",
    "completely lost (and train a model that maximizes the probability of conversion):\n",
    "\n",
    "Offer —> Purchase\n",
    "  |\n",
    "  ∨\n",
    "Lost costumer\n",
    "\n",
    "However, the real retail environment is more complex and customers can interact with a \n",
    "retailer multiple times before a conversion occurs, and the customer makes related purchases.\n",
    "\n",
    "For instance, in a situation where the actions in the strategy are related, and their order \n",
    "is important, the initial action alone may not increase conversions, but it can boost the \n",
    "efficiency of the downstream actions. \n",
    "\n",
    "It can also be the case that an action is most efficient if issued not in the beginning, but\n",
    "between two other different actions, and so on.\n",
    "\n",
    "In complex structures of a customer journey, as customer maturity grows over time, offerings\n",
    "need to be sequenced properly to account for this.\n",
    "\n",
    "MARKOV DECISION PROCESSES (MDP):\n",
    "Use cases can frequently be represented as Markov Decision Processes (MDP), where a customer\n",
    "can potentially be in several different states and move from one state to another over time \n",
    "under the influence of marketing actions. \n",
    "\n",
    "In each state, the marketer has to choose an action (or non-action) to execute, and each \n",
    "transition between states corresponds to some reward (e.g. number of purchases), so that all\n",
    "rewards along the customer trajectory sum up to a total return that corresponds to customer \n",
    "LTV or campaign ROI.\n",
    "\n",
    "Although we can use a hand-crafted set of states with clear semantic meaning, we will assume\n",
    "the state is represented simply by a customer feature vector, so that the total number of \n",
    "states can be large or infinite in the case of real-valued features. Note that the state at \n",
    "any point of time can include records of all previous actions taken with regard to this \n",
    "customer.\n",
    "\n",
    "In the MDP framework, our goal is to find an optimal policy — π — that maps each state to the \n",
    "probabilities of selecting each possible action, so that π(a | s) is the probability of \n",
    "taking action — a — given that a customer is currently in state — s. The optimality of the \n",
    "policy can be quantified using the expected return under this policy – the expected sum of \n",
    "rewards — r — earned at each transition.\n",
    "\n",
    "A naive solution for this problem is thus to build multiple look-alike models for different \n",
    "actions and different designs of the training labels.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
